{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375d2158",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b380fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.1+cu130\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8ec2b",
   "metadata": {},
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2d65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551e797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world,', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello world, This is a test.\"\n",
    "result = re.split(f'(\\s)', text) # removes white spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce41b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', ',', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)',text) # remove ',' '.' and white spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b153c9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', ',', 'This', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip white spaces from each item and then filter out any empty spaces\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9044311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', ';', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this--; a test?\"\n",
    "result = re.split(f'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fb9b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(f'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614061f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4953c2e",
   "metadata": {},
   "source": [
    "## Converting tokens in token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaecc063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d937d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc78e59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e024d0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!',\n",
       " 1: '\"',\n",
       " 2: \"'\",\n",
       " 3: '(',\n",
       " 4: ')',\n",
       " 5: ',',\n",
       " 6: '--',\n",
       " 7: '.',\n",
       " 8: ':',\n",
       " 9: ';',\n",
       " 10: '?',\n",
       " 11: 'A',\n",
       " 12: 'Ah',\n",
       " 13: 'Among',\n",
       " 14: 'And',\n",
       " 15: 'Are',\n",
       " 16: 'Arrt',\n",
       " 17: 'As',\n",
       " 18: 'At',\n",
       " 19: 'Be',\n",
       " 20: 'Begin',\n",
       " 21: 'Burlington',\n",
       " 22: 'But',\n",
       " 23: 'By',\n",
       " 24: 'Carlo',\n",
       " 25: 'Chicago',\n",
       " 26: 'Claude',\n",
       " 27: 'Come',\n",
       " 28: 'Croft',\n",
       " 29: 'Destroyed',\n",
       " 30: 'Devonshire',\n",
       " 31: 'Don',\n",
       " 32: 'Dubarry',\n",
       " 33: 'Emperors',\n",
       " 34: 'Florence',\n",
       " 35: 'For',\n",
       " 36: 'Gallery',\n",
       " 37: 'Gideon',\n",
       " 38: 'Gisburn',\n",
       " 39: 'Gisburns',\n",
       " 40: 'Grafton',\n",
       " 41: 'Greek',\n",
       " 42: 'Grindle',\n",
       " 43: 'Grindles',\n",
       " 44: 'HAD',\n",
       " 45: 'Had',\n",
       " 46: 'Hang',\n",
       " 47: 'Has',\n",
       " 48: 'He',\n",
       " 49: 'Her',\n",
       " 50: 'Hermia',\n",
       " 51: 'His',\n",
       " 52: 'How',\n",
       " 53: 'I',\n",
       " 54: 'If',\n",
       " 55: 'In',\n",
       " 56: 'It',\n",
       " 57: 'Jack',\n",
       " 58: 'Jove',\n",
       " 59: 'Just',\n",
       " 60: 'Lord',\n",
       " 61: 'Made',\n",
       " 62: 'Miss',\n",
       " 63: 'Money',\n",
       " 64: 'Monte',\n",
       " 65: 'Moon-dancers',\n",
       " 66: 'Mr',\n",
       " 67: 'Mrs',\n",
       " 68: 'My',\n",
       " 69: 'Never',\n",
       " 70: 'No',\n",
       " 71: 'Now',\n",
       " 72: 'Nutley',\n",
       " 73: 'Of',\n",
       " 74: 'Oh',\n",
       " 75: 'On',\n",
       " 76: 'Once',\n",
       " 77: 'Only',\n",
       " 78: 'Or',\n",
       " 79: 'Perhaps',\n",
       " 80: 'Poor',\n",
       " 81: 'Professional',\n",
       " 82: 'Renaissance',\n",
       " 83: 'Rickham',\n",
       " 84: 'Riviera',\n",
       " 85: 'Rome',\n",
       " 86: 'Russian',\n",
       " 87: 'Sevres',\n",
       " 88: 'She',\n",
       " 89: 'Stroud',\n",
       " 90: 'Strouds',\n",
       " 91: 'Suddenly',\n",
       " 92: 'That',\n",
       " 93: 'The',\n",
       " 94: 'Then',\n",
       " 95: 'There',\n",
       " 96: 'They',\n",
       " 97: 'This',\n",
       " 98: 'Those',\n",
       " 99: 'Though',\n",
       " 100: 'Thwing',\n",
       " 101: 'Thwings',\n",
       " 102: 'To',\n",
       " 103: 'Usually',\n",
       " 104: 'Venetian',\n",
       " 105: 'Victor',\n",
       " 106: 'Was',\n",
       " 107: 'We',\n",
       " 108: 'Well',\n",
       " 109: 'What',\n",
       " 110: 'When',\n",
       " 111: 'Why',\n",
       " 112: 'Yes',\n",
       " 113: 'You',\n",
       " 114: '_',\n",
       " 115: 'a',\n",
       " 116: 'abdication',\n",
       " 117: 'able',\n",
       " 118: 'about',\n",
       " 119: 'above',\n",
       " 120: 'abruptly',\n",
       " 121: 'absolute',\n",
       " 122: 'absorbed',\n",
       " 123: 'absurdity',\n",
       " 124: 'academic',\n",
       " 125: 'accuse',\n",
       " 126: 'accustomed',\n",
       " 127: 'across',\n",
       " 128: 'activity',\n",
       " 129: 'add',\n",
       " 130: 'added',\n",
       " 131: 'admirers',\n",
       " 132: 'adopted',\n",
       " 133: 'adulation',\n",
       " 134: 'advance',\n",
       " 135: 'aesthetic',\n",
       " 136: 'affect',\n",
       " 137: 'afraid',\n",
       " 138: 'after',\n",
       " 139: 'afterward',\n",
       " 140: 'again',\n",
       " 141: 'ago',\n",
       " 142: 'ah',\n",
       " 143: 'air',\n",
       " 144: 'alive',\n",
       " 145: 'all',\n",
       " 146: 'almost',\n",
       " 147: 'alone',\n",
       " 148: 'along',\n",
       " 149: 'always',\n",
       " 150: 'am',\n",
       " 151: 'amazement',\n",
       " 152: 'amid',\n",
       " 153: 'among',\n",
       " 154: 'amplest',\n",
       " 155: 'amusing',\n",
       " 156: 'an',\n",
       " 157: 'and',\n",
       " 158: 'another',\n",
       " 159: 'answer',\n",
       " 160: 'answered',\n",
       " 161: 'any',\n",
       " 162: 'anything',\n",
       " 163: 'anywhere',\n",
       " 164: 'apparent',\n",
       " 165: 'apparently',\n",
       " 166: 'appearance',\n",
       " 167: 'appeared',\n",
       " 168: 'appointed',\n",
       " 169: 'are',\n",
       " 170: 'arm',\n",
       " 171: 'arm-chair',\n",
       " 172: 'arm-chairs',\n",
       " 173: 'arms',\n",
       " 174: 'art',\n",
       " 175: 'articles',\n",
       " 176: 'artist',\n",
       " 177: 'as',\n",
       " 178: 'aside',\n",
       " 179: 'asked',\n",
       " 180: 'at',\n",
       " 181: 'atmosphere',\n",
       " 182: 'atom',\n",
       " 183: 'attack',\n",
       " 184: 'attention',\n",
       " 185: 'attitude',\n",
       " 186: 'audacities',\n",
       " 187: 'away',\n",
       " 188: 'awful',\n",
       " 189: 'axioms',\n",
       " 190: 'azaleas',\n",
       " 191: 'back',\n",
       " 192: 'background',\n",
       " 193: 'balance',\n",
       " 194: 'balancing',\n",
       " 195: 'balustraded',\n",
       " 196: 'basking',\n",
       " 197: 'bath-rooms',\n",
       " 198: 'be',\n",
       " 199: 'beaming',\n",
       " 200: 'bean-stalk',\n",
       " 201: 'bear',\n",
       " 202: 'beard',\n",
       " 203: 'beauty',\n",
       " 204: 'became',\n",
       " 205: 'because',\n",
       " 206: 'becoming',\n",
       " 207: 'bed',\n",
       " 208: 'been',\n",
       " 209: 'before',\n",
       " 210: 'began',\n",
       " 211: 'begun',\n",
       " 212: 'behind',\n",
       " 213: 'being',\n",
       " 214: 'believed',\n",
       " 215: 'beneath',\n",
       " 216: 'bespoke',\n",
       " 217: 'better',\n",
       " 218: 'between',\n",
       " 219: 'big',\n",
       " 220: 'bits',\n",
       " 221: 'bitterness',\n",
       " 222: 'blocked',\n",
       " 223: 'born',\n",
       " 224: 'borne',\n",
       " 225: 'boudoir',\n",
       " 226: 'bravura',\n",
       " 227: 'break',\n",
       " 228: 'breaking',\n",
       " 229: 'breathing',\n",
       " 230: 'bric-a-brac',\n",
       " 231: 'briefly',\n",
       " 232: 'brings',\n",
       " 233: 'bronzes',\n",
       " 234: 'brought',\n",
       " 235: 'brown',\n",
       " 236: 'brush',\n",
       " 237: 'bull',\n",
       " 238: 'business',\n",
       " 239: 'but',\n",
       " 240: 'buying',\n",
       " 241: 'by',\n",
       " 242: 'called',\n",
       " 243: 'came',\n",
       " 244: 'can',\n",
       " 245: 'canvas',\n",
       " 246: 'canvases',\n",
       " 247: 'cards',\n",
       " 248: 'care',\n",
       " 249: 'career',\n",
       " 250: 'caught',\n",
       " 251: 'central',\n",
       " 252: 'chair',\n",
       " 253: 'chap',\n",
       " 254: 'characteristic',\n",
       " 255: 'charming',\n",
       " 256: 'cheap',\n",
       " 257: 'check',\n",
       " 258: 'cheeks',\n",
       " 259: 'chest',\n",
       " 260: 'chimney-piece',\n",
       " 261: 'chucked',\n",
       " 262: 'cigar',\n",
       " 263: 'cigarette',\n",
       " 264: 'cigars',\n",
       " 265: 'circulation',\n",
       " 266: 'circumstance',\n",
       " 267: 'circus-clown',\n",
       " 268: 'claimed',\n",
       " 269: 'clasping',\n",
       " 270: 'clear',\n",
       " 271: 'cleverer',\n",
       " 272: 'close',\n",
       " 273: 'clue',\n",
       " 274: 'coat',\n",
       " 275: 'collapsed',\n",
       " 276: 'colour',\n",
       " 277: 'come',\n",
       " 278: 'comfortable',\n",
       " 279: 'coming',\n",
       " 280: 'companion',\n",
       " 281: 'compared',\n",
       " 282: 'complex',\n",
       " 283: 'confident',\n",
       " 284: 'congesting',\n",
       " 285: 'conjugal',\n",
       " 286: 'constraint',\n",
       " 287: 'consummate',\n",
       " 288: 'contended',\n",
       " 289: 'continued',\n",
       " 290: 'corner',\n",
       " 291: 'corrected',\n",
       " 292: 'could',\n",
       " 293: 'couldn',\n",
       " 294: 'count',\n",
       " 295: 'countenance',\n",
       " 296: 'couple',\n",
       " 297: 'course',\n",
       " 298: 'covered',\n",
       " 299: 'craft',\n",
       " 300: 'cried',\n",
       " 301: 'crossed',\n",
       " 302: 'crowned',\n",
       " 303: 'crumbled',\n",
       " 304: 'cry',\n",
       " 305: 'cured',\n",
       " 306: 'curiosity',\n",
       " 307: 'curious',\n",
       " 308: 'current',\n",
       " 309: 'curtains',\n",
       " 310: 'd',\n",
       " 311: 'dabble',\n",
       " 312: 'damask',\n",
       " 313: 'dark',\n",
       " 314: 'dashed',\n",
       " 315: 'day',\n",
       " 316: 'days',\n",
       " 317: 'dead',\n",
       " 318: 'deadening',\n",
       " 319: 'dear',\n",
       " 320: 'deep',\n",
       " 321: 'deerhound',\n",
       " 322: 'degree',\n",
       " 323: 'delicate',\n",
       " 324: 'demand',\n",
       " 325: 'denied',\n",
       " 326: 'deploring',\n",
       " 327: 'deprecating',\n",
       " 328: 'deprecatingly',\n",
       " 329: 'desire',\n",
       " 330: 'destroyed',\n",
       " 331: 'destruction',\n",
       " 332: 'desultory',\n",
       " 333: 'detail',\n",
       " 334: 'diagnosis',\n",
       " 335: 'did',\n",
       " 336: 'didn',\n",
       " 337: 'died',\n",
       " 338: 'dim',\n",
       " 339: 'dimmest',\n",
       " 340: 'dingy',\n",
       " 341: 'dining-room',\n",
       " 342: 'disarming',\n",
       " 343: 'discovery',\n",
       " 344: 'discrimination',\n",
       " 345: 'discussion',\n",
       " 346: 'disdain',\n",
       " 347: 'disdained',\n",
       " 348: 'disease',\n",
       " 349: 'disguised',\n",
       " 350: 'display',\n",
       " 351: 'dissatisfied',\n",
       " 352: 'distinguished',\n",
       " 353: 'distract',\n",
       " 354: 'divert',\n",
       " 355: 'do',\n",
       " 356: 'doesn',\n",
       " 357: 'doing',\n",
       " 358: 'domestic',\n",
       " 359: 'don',\n",
       " 360: 'done',\n",
       " 361: 'donkey',\n",
       " 362: 'down',\n",
       " 363: 'dozen',\n",
       " 364: 'dragged',\n",
       " 365: 'drawing-room',\n",
       " 366: 'drawing-rooms',\n",
       " 367: 'drawn',\n",
       " 368: 'dress-closets',\n",
       " 369: 'drew',\n",
       " 370: 'dropped',\n",
       " 371: 'each',\n",
       " 372: 'earth',\n",
       " 373: 'ease',\n",
       " 374: 'easel',\n",
       " 375: 'easy',\n",
       " 376: 'echoed',\n",
       " 377: 'economy',\n",
       " 378: 'effect',\n",
       " 379: 'effects',\n",
       " 380: 'efforts',\n",
       " 381: 'egregious',\n",
       " 382: 'eighteenth-century',\n",
       " 383: 'elbow',\n",
       " 384: 'elegant',\n",
       " 385: 'else',\n",
       " 386: 'embarrassed',\n",
       " 387: 'enabled',\n",
       " 388: 'end',\n",
       " 389: 'endless',\n",
       " 390: 'enjoy',\n",
       " 391: 'enlightenment',\n",
       " 392: 'enough',\n",
       " 393: 'ensuing',\n",
       " 394: 'equally',\n",
       " 395: 'equanimity',\n",
       " 396: 'escape',\n",
       " 397: 'established',\n",
       " 398: 'etching',\n",
       " 399: 'even',\n",
       " 400: 'event',\n",
       " 401: 'ever',\n",
       " 402: 'everlasting',\n",
       " 403: 'every',\n",
       " 404: 'exasperated',\n",
       " 405: 'except',\n",
       " 406: 'excuse',\n",
       " 407: 'excusing',\n",
       " 408: 'existed',\n",
       " 409: 'expected',\n",
       " 410: 'exquisite',\n",
       " 411: 'exquisitely',\n",
       " 412: 'extenuation',\n",
       " 413: 'exterminating',\n",
       " 414: 'extracting',\n",
       " 415: 'eye',\n",
       " 416: 'eyebrows',\n",
       " 417: 'eyes',\n",
       " 418: 'face',\n",
       " 419: 'faces',\n",
       " 420: 'fact',\n",
       " 421: 'faded',\n",
       " 422: 'failed',\n",
       " 423: 'failure',\n",
       " 424: 'fair',\n",
       " 425: 'faith',\n",
       " 426: 'false',\n",
       " 427: 'familiar',\n",
       " 428: 'famille-verte',\n",
       " 429: 'fancy',\n",
       " 430: 'fashionable',\n",
       " 431: 'fate',\n",
       " 432: 'feather',\n",
       " 433: 'feet',\n",
       " 434: 'fell',\n",
       " 435: 'fellow',\n",
       " 436: 'felt',\n",
       " 437: 'few',\n",
       " 438: 'fewer',\n",
       " 439: 'finality',\n",
       " 440: 'find',\n",
       " 441: 'fingers',\n",
       " 442: 'first',\n",
       " 443: 'fit',\n",
       " 444: 'fitting',\n",
       " 445: 'five',\n",
       " 446: 'flash',\n",
       " 447: 'flashed',\n",
       " 448: 'florid',\n",
       " 449: 'flowers',\n",
       " 450: 'fluently',\n",
       " 451: 'flung',\n",
       " 452: 'follow',\n",
       " 453: 'followed',\n",
       " 454: 'fond',\n",
       " 455: 'footstep',\n",
       " 456: 'for',\n",
       " 457: 'forced',\n",
       " 458: 'forcing',\n",
       " 459: 'forehead',\n",
       " 460: 'foreign',\n",
       " 461: 'foreseen',\n",
       " 462: 'forgive',\n",
       " 463: 'forgotten',\n",
       " 464: 'form',\n",
       " 465: 'formed',\n",
       " 466: 'forming',\n",
       " 467: 'forward',\n",
       " 468: 'fostered',\n",
       " 469: 'found',\n",
       " 470: 'foundations',\n",
       " 471: 'fragment',\n",
       " 472: 'fragments',\n",
       " 473: 'frame',\n",
       " 474: 'frames',\n",
       " 475: 'frequently',\n",
       " 476: 'friend',\n",
       " 477: 'from',\n",
       " 478: 'full',\n",
       " 479: 'fullest',\n",
       " 480: 'furiously',\n",
       " 481: 'furrowed',\n",
       " 482: 'garlanded',\n",
       " 483: 'garlands',\n",
       " 484: 'gave',\n",
       " 485: 'genial',\n",
       " 486: 'genius',\n",
       " 487: 'gesture',\n",
       " 488: 'get',\n",
       " 489: 'getting',\n",
       " 490: 'give',\n",
       " 491: 'given',\n",
       " 492: 'glad',\n",
       " 493: 'glanced',\n",
       " 494: 'glimpse',\n",
       " 495: 'gloried',\n",
       " 496: 'glory',\n",
       " 497: 'go',\n",
       " 498: 'going',\n",
       " 499: 'gone',\n",
       " 500: 'good',\n",
       " 501: 'good-breeding',\n",
       " 502: 'good-humoured',\n",
       " 503: 'got',\n",
       " 504: 'grace',\n",
       " 505: 'gradually',\n",
       " 506: 'gray',\n",
       " 507: 'grayish',\n",
       " 508: 'great',\n",
       " 509: 'greatest',\n",
       " 510: 'greatness',\n",
       " 511: 'grew',\n",
       " 512: 'groping',\n",
       " 513: 'growing',\n",
       " 514: 'had',\n",
       " 515: 'hadn',\n",
       " 516: 'hair',\n",
       " 517: 'half',\n",
       " 518: 'half-light',\n",
       " 519: 'half-mechanically',\n",
       " 520: 'hall',\n",
       " 521: 'hand',\n",
       " 522: 'hands',\n",
       " 523: 'handsome',\n",
       " 524: 'hanging',\n",
       " 525: 'happen',\n",
       " 526: 'happened',\n",
       " 527: 'hard',\n",
       " 528: 'hardly',\n",
       " 529: 'has',\n",
       " 530: 'have',\n",
       " 531: 'haven',\n",
       " 532: 'having',\n",
       " 533: 'he',\n",
       " 534: 'head',\n",
       " 535: 'hear',\n",
       " 536: 'heard',\n",
       " 537: 'heart',\n",
       " 538: 'height',\n",
       " 539: 'her',\n",
       " 540: 'here',\n",
       " 541: 'hermit',\n",
       " 542: 'herself',\n",
       " 543: 'hesitations',\n",
       " 544: 'hide',\n",
       " 545: 'high',\n",
       " 546: 'him',\n",
       " 547: 'himself',\n",
       " 548: 'hint',\n",
       " 549: 'his',\n",
       " 550: 'history',\n",
       " 551: 'holding',\n",
       " 552: 'home',\n",
       " 553: 'honour',\n",
       " 554: 'hooded',\n",
       " 555: 'hostess',\n",
       " 556: 'hot-house',\n",
       " 557: 'hour',\n",
       " 558: 'hours',\n",
       " 559: 'house',\n",
       " 560: 'how',\n",
       " 561: 'hung',\n",
       " 562: 'husband',\n",
       " 563: 'idea',\n",
       " 564: 'idle',\n",
       " 565: 'idling',\n",
       " 566: 'if',\n",
       " 567: 'immediately',\n",
       " 568: 'in',\n",
       " 569: 'incense',\n",
       " 570: 'indifferent',\n",
       " 571: 'inevitable',\n",
       " 572: 'inevitably',\n",
       " 573: 'inflexible',\n",
       " 574: 'insensible',\n",
       " 575: 'insignificant',\n",
       " 576: 'instinctively',\n",
       " 577: 'instructive',\n",
       " 578: 'interesting',\n",
       " 579: 'into',\n",
       " 580: 'ironic',\n",
       " 581: 'irony',\n",
       " 582: 'irrelevance',\n",
       " 583: 'irrevocable',\n",
       " 584: 'is',\n",
       " 585: 'it',\n",
       " 586: 'its',\n",
       " 587: 'itself',\n",
       " 588: 'jardiniere',\n",
       " 589: 'jealousy',\n",
       " 590: 'just',\n",
       " 591: 'keep',\n",
       " 592: 'kept',\n",
       " 593: 'kind',\n",
       " 594: 'knees',\n",
       " 595: 'knew',\n",
       " 596: 'know',\n",
       " 597: 'known',\n",
       " 598: 'laid',\n",
       " 599: 'lair',\n",
       " 600: 'landing',\n",
       " 601: 'language',\n",
       " 602: 'last',\n",
       " 603: 'late',\n",
       " 604: 'later',\n",
       " 605: 'latter',\n",
       " 606: 'laugh',\n",
       " 607: 'laughed',\n",
       " 608: 'lay',\n",
       " 609: 'leading',\n",
       " 610: 'lean',\n",
       " 611: 'learned',\n",
       " 612: 'least',\n",
       " 613: 'leathery',\n",
       " 614: 'leave',\n",
       " 615: 'led',\n",
       " 616: 'left',\n",
       " 617: 'leisure',\n",
       " 618: 'lends',\n",
       " 619: 'lent',\n",
       " 620: 'let',\n",
       " 621: 'lies',\n",
       " 622: 'life',\n",
       " 623: 'life-likeness',\n",
       " 624: 'lift',\n",
       " 625: 'lifted',\n",
       " 626: 'light',\n",
       " 627: 'lightly',\n",
       " 628: 'like',\n",
       " 629: 'liked',\n",
       " 630: 'line',\n",
       " 631: 'lines',\n",
       " 632: 'lingered',\n",
       " 633: 'lips',\n",
       " 634: 'lit',\n",
       " 635: 'little',\n",
       " 636: 'live',\n",
       " 637: 'll',\n",
       " 638: 'loathing',\n",
       " 639: 'long',\n",
       " 640: 'longed',\n",
       " 641: 'longer',\n",
       " 642: 'look',\n",
       " 643: 'looked',\n",
       " 644: 'looking',\n",
       " 645: 'lose',\n",
       " 646: 'loss',\n",
       " 647: 'lounging',\n",
       " 648: 'lovely',\n",
       " 649: 'lucky',\n",
       " 650: 'lump',\n",
       " 651: 'luncheon-table',\n",
       " 652: 'luxury',\n",
       " 653: 'lying',\n",
       " 654: 'made',\n",
       " 655: 'make',\n",
       " 656: 'man',\n",
       " 657: 'manage',\n",
       " 658: 'managed',\n",
       " 659: 'mantel-piece',\n",
       " 660: 'marble',\n",
       " 661: 'married',\n",
       " 662: 'may',\n",
       " 663: 'me',\n",
       " 664: 'meant',\n",
       " 665: 'mediocrity',\n",
       " 666: 'medium',\n",
       " 667: 'mentioned',\n",
       " 668: 'mere',\n",
       " 669: 'merely',\n",
       " 670: 'met',\n",
       " 671: 'might',\n",
       " 672: 'mighty',\n",
       " 673: 'millionaire',\n",
       " 674: 'mine',\n",
       " 675: 'minute',\n",
       " 676: 'minutes',\n",
       " 677: 'mirrors',\n",
       " 678: 'modest',\n",
       " 679: 'modesty',\n",
       " 680: 'moment',\n",
       " 681: 'money',\n",
       " 682: 'monumental',\n",
       " 683: 'mood',\n",
       " 684: 'morbidly',\n",
       " 685: 'more',\n",
       " 686: 'most',\n",
       " 687: 'mourn',\n",
       " 688: 'mourned',\n",
       " 689: 'moustache',\n",
       " 690: 'moved',\n",
       " 691: 'much',\n",
       " 692: 'muddling',\n",
       " 693: 'multiplied',\n",
       " 694: 'murmur',\n",
       " 695: 'muscles',\n",
       " 696: 'must',\n",
       " 697: 'my',\n",
       " 698: 'myself',\n",
       " 699: 'mysterious',\n",
       " 700: 'naive',\n",
       " 701: 'near',\n",
       " 702: 'nearly',\n",
       " 703: 'negatived',\n",
       " 704: 'nervous',\n",
       " 705: 'nervousness',\n",
       " 706: 'neutral',\n",
       " 707: 'never',\n",
       " 708: 'next',\n",
       " 709: 'no',\n",
       " 710: 'none',\n",
       " 711: 'not',\n",
       " 712: 'note',\n",
       " 713: 'nothing',\n",
       " 714: 'now',\n",
       " 715: 'nymphs',\n",
       " 716: 'oak',\n",
       " 717: 'obituary',\n",
       " 718: 'object',\n",
       " 719: 'objects',\n",
       " 720: 'occurred',\n",
       " 721: 'oddly',\n",
       " 722: 'of',\n",
       " 723: 'off',\n",
       " 724: 'often',\n",
       " 725: 'oh',\n",
       " 726: 'old',\n",
       " 727: 'on',\n",
       " 728: 'once',\n",
       " 729: 'one',\n",
       " 730: 'ones',\n",
       " 731: 'only',\n",
       " 732: 'onto',\n",
       " 733: 'open',\n",
       " 734: 'or',\n",
       " 735: 'other',\n",
       " 736: 'our',\n",
       " 737: 'ourselves',\n",
       " 738: 'out',\n",
       " 739: 'outline',\n",
       " 740: 'oval',\n",
       " 741: 'over',\n",
       " 742: 'own',\n",
       " 743: 'packed',\n",
       " 744: 'paid',\n",
       " 745: 'paint',\n",
       " 746: 'painted',\n",
       " 747: 'painter',\n",
       " 748: 'painting',\n",
       " 749: 'pale',\n",
       " 750: 'paled',\n",
       " 751: 'palm-trees',\n",
       " 752: 'panel',\n",
       " 753: 'panelling',\n",
       " 754: 'pardonable',\n",
       " 755: 'pardoned',\n",
       " 756: 'part',\n",
       " 757: 'passages',\n",
       " 758: 'passing',\n",
       " 759: 'past',\n",
       " 760: 'pastels',\n",
       " 761: 'pathos',\n",
       " 762: 'patient',\n",
       " 763: 'people',\n",
       " 764: 'perceptible',\n",
       " 765: 'perfect',\n",
       " 766: 'persistence',\n",
       " 767: 'persuasively',\n",
       " 768: 'phrase',\n",
       " 769: 'picture',\n",
       " 770: 'pictures',\n",
       " 771: 'pines',\n",
       " 772: 'pink',\n",
       " 773: 'place',\n",
       " 774: 'placed',\n",
       " 775: 'plain',\n",
       " 776: 'platitudes',\n",
       " 777: 'pleased',\n",
       " 778: 'pockets',\n",
       " 779: 'point',\n",
       " 780: 'poised',\n",
       " 781: 'poor',\n",
       " 782: 'portrait',\n",
       " 783: 'posing',\n",
       " 784: 'possessed',\n",
       " 785: 'poverty',\n",
       " 786: 'predicted',\n",
       " 787: 'preliminary',\n",
       " 788: 'presenting',\n",
       " 789: 'prestidigitation',\n",
       " 790: 'pretty',\n",
       " 791: 'previous',\n",
       " 792: 'price',\n",
       " 793: 'pride',\n",
       " 794: 'princely',\n",
       " 795: 'prism',\n",
       " 796: 'problem',\n",
       " 797: 'proclaiming',\n",
       " 798: 'prodigious',\n",
       " 799: 'profusion',\n",
       " 800: 'protest',\n",
       " 801: 'prove',\n",
       " 802: 'public',\n",
       " 803: 'purblind',\n",
       " 804: 'purely',\n",
       " 805: 'pushed',\n",
       " 806: 'put',\n",
       " 807: 'qualities',\n",
       " 808: 'quality',\n",
       " 809: 'queerly',\n",
       " 810: 'question',\n",
       " 811: 'quickly',\n",
       " 812: 'quietly',\n",
       " 813: 'quite',\n",
       " 814: 'quote',\n",
       " 815: 'rain',\n",
       " 816: 'raised',\n",
       " 817: 'random',\n",
       " 818: 'rather',\n",
       " 819: 're',\n",
       " 820: 'real',\n",
       " 821: 'really',\n",
       " 822: 'reared',\n",
       " 823: 'reason',\n",
       " 824: 'reassurance',\n",
       " 825: 'recovering',\n",
       " 826: 'recreated',\n",
       " 827: 'reflected',\n",
       " 828: 'reflection',\n",
       " 829: 'regrets',\n",
       " 830: 'relatively',\n",
       " 831: 'remained',\n",
       " 832: 'remember',\n",
       " 833: 'reminded',\n",
       " 834: 'repeating',\n",
       " 835: 'represented',\n",
       " 836: 'reproduction',\n",
       " 837: 'resented',\n",
       " 838: 'resolve',\n",
       " 839: 'resources',\n",
       " 840: 'rest',\n",
       " 841: 'rich',\n",
       " 842: 'ridiculous',\n",
       " 843: 'robbed',\n",
       " 844: 'romantic',\n",
       " 845: 'room',\n",
       " 846: 'rose',\n",
       " 847: 'rs',\n",
       " 848: 'rule',\n",
       " 849: 'run',\n",
       " 850: 's',\n",
       " 851: 'said',\n",
       " 852: 'same',\n",
       " 853: 'satisfaction',\n",
       " 854: 'savour',\n",
       " 855: 'saw',\n",
       " 856: 'say',\n",
       " 857: 'saying',\n",
       " 858: 'says',\n",
       " 859: 'scorn',\n",
       " 860: 'scornful',\n",
       " 861: 'secret',\n",
       " 862: 'see',\n",
       " 863: 'seemed',\n",
       " 864: 'seen',\n",
       " 865: 'self-confident',\n",
       " 866: 'send',\n",
       " 867: 'sensation',\n",
       " 868: 'sensitive',\n",
       " 869: 'sent',\n",
       " 870: 'serious',\n",
       " 871: 'set',\n",
       " 872: 'sex',\n",
       " 873: 'shade',\n",
       " 874: 'shaking',\n",
       " 875: 'shall',\n",
       " 876: 'she',\n",
       " 877: 'shirked',\n",
       " 878: 'short',\n",
       " 879: 'should',\n",
       " 880: 'shoulder',\n",
       " 881: 'shoulders',\n",
       " 882: 'show',\n",
       " 883: 'showed',\n",
       " 884: 'showy',\n",
       " 885: 'shrug',\n",
       " 886: 'shrugged',\n",
       " 887: 'sight',\n",
       " 888: 'sign',\n",
       " 889: 'silent',\n",
       " 890: 'silver',\n",
       " 891: 'similar',\n",
       " 892: 'simpleton',\n",
       " 893: 'simplifications',\n",
       " 894: 'simply',\n",
       " 895: 'since',\n",
       " 896: 'single',\n",
       " 897: 'sitter',\n",
       " 898: 'sitters',\n",
       " 899: 'sketch',\n",
       " 900: 'skill',\n",
       " 901: 'slight',\n",
       " 902: 'slightly',\n",
       " 903: 'slowly',\n",
       " 904: 'small',\n",
       " 905: 'smile',\n",
       " 906: 'smiling',\n",
       " 907: 'sneer',\n",
       " 908: 'so',\n",
       " 909: 'solace',\n",
       " 910: 'some',\n",
       " 911: 'somebody',\n",
       " 912: 'something',\n",
       " 913: 'spacious',\n",
       " 914: 'spaniel',\n",
       " 915: 'speaking-tubes',\n",
       " 916: 'speculations',\n",
       " 917: 'spite',\n",
       " 918: 'splash',\n",
       " 919: 'square',\n",
       " 920: 'stairs',\n",
       " 921: 'stammer',\n",
       " 922: 'stand',\n",
       " 923: 'standing',\n",
       " 924: 'started',\n",
       " 925: 'stay',\n",
       " 926: 'still',\n",
       " 927: 'stocked',\n",
       " 928: 'stood',\n",
       " 929: 'stopped',\n",
       " 930: 'stopping',\n",
       " 931: 'straddling',\n",
       " 932: 'straight',\n",
       " 933: 'strain',\n",
       " 934: 'straining',\n",
       " 935: 'strange',\n",
       " 936: 'straw',\n",
       " 937: 'stream',\n",
       " 938: 'stroke',\n",
       " 939: 'strokes',\n",
       " 940: 'strolled',\n",
       " 941: 'strongest',\n",
       " 942: 'strongly',\n",
       " 943: 'struck',\n",
       " 944: 'studio',\n",
       " 945: 'stuff',\n",
       " 946: 'subject',\n",
       " 947: 'substantial',\n",
       " 948: 'suburban',\n",
       " 949: 'such',\n",
       " 950: 'suddenly',\n",
       " 951: 'suffered',\n",
       " 952: 'sugar',\n",
       " 953: 'suggested',\n",
       " 954: 'sunburn',\n",
       " 955: 'sunburnt',\n",
       " 956: 'sunlit',\n",
       " 957: 'superb',\n",
       " 958: 'sure',\n",
       " 959: 'surest',\n",
       " 960: 'surface',\n",
       " 961: 'surprise',\n",
       " 962: 'surprised',\n",
       " 963: 'surrounded',\n",
       " 964: 'suspected',\n",
       " 965: 'sweetly',\n",
       " 966: 'sweetness',\n",
       " 967: 'swelling',\n",
       " 968: 'swept',\n",
       " 969: 'swum',\n",
       " 970: 't',\n",
       " 971: 'table',\n",
       " 972: 'take',\n",
       " 973: 'taken',\n",
       " 974: 'talking',\n",
       " 975: 'tea',\n",
       " 976: 'tears',\n",
       " 977: 'technicalities',\n",
       " 978: 'technique',\n",
       " 979: 'tell',\n",
       " 980: 'tells',\n",
       " 981: 'tempting',\n",
       " 982: 'terra-cotta',\n",
       " 983: 'terrace',\n",
       " 984: 'terraces',\n",
       " 985: 'terribly',\n",
       " 986: 'than',\n",
       " 987: 'that',\n",
       " 988: 'the',\n",
       " 989: 'their',\n",
       " 990: 'them',\n",
       " 991: 'then',\n",
       " 992: 'there',\n",
       " 993: 'therefore',\n",
       " 994: 'they',\n",
       " 995: 'thin',\n",
       " 996: 'thing',\n",
       " 997: 'things',\n",
       " 998: 'think',\n",
       " 999: 'this',\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i:s for s,i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd1072ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Function turns text into token IDs\n",
    "        \"\"\"\n",
    "        preprocessed = re.split(f'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Function turns tokens IDs back into text \n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb33b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec56f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afa93b",
   "metadata": {},
   "source": [
    "## Adding special context tokens \n",
    "\n",
    "- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n",
    "- Some tokenizers use special tokens to help the LLM with additional context\n",
    "- Some of these special tokens are\n",
    "  - `[BOS]` (beginning of sequence) marks the beginning of text\n",
    "  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
    "  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
    "- `[UNK]` to represent words that are not included in the vocabulary\n",
    "\n",
    "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
    "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
    "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
    "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21fcce08",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = SimpleTokenizerV1(vocab)\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     11\u001b[39m preprocessed = re.split(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     12\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ids = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     11\u001b[39m preprocessed = re.split(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     12\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "168d425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c13aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb8d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abbbc42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d472e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8259a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ab18f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf25105",
   "metadata": {},
   "source": [
    "## BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b55714",
   "metadata": {},
   "source": [
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
    "- I created a notebook in the [./bytepair_encoder](../02_bonus_bytepair-encoder) that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0e04349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7969bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2a3308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "679406b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfbaa2",
   "metadata": {},
   "source": [
    "## Data Sampling with a sliding window\n",
    "\n",
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d47919e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",'r',encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ad36eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197, 832, 262, 46475, 286, 18113, 544, 338, 10953, 314, 2936, 1498, 284, 1986, 262, 1109, 351, 1602, 11227, 414, 13, 23676, 3619, 402, 271, 10899, 0, 383, 1466, 550, 925, 683, 438, 270, 373, 15830, 326, 484, 815, 25722, 683, 13, 9754, 465, 898, 1714, 7380, 30090, 547, 2982, 11, 290, 287, 465, 898, 3292, 8941, 257, 4636, 28582, 13, 18612, 35394, 30, 8673, 13, 1002, 340, 547, 11, 262, 15393, 286, 262, 5977, 373, 29178, 3474, 416, 1310, 40559, 11959, 1636, 11, 508, 11, 287, 477, 922, 4562, 11, 3181, 503, 287, 262, 37090, 257, 845, 22665, 366, 672, 270, 2838, 1, 319, 3619, 438, 505, 286, 883, 905, 88, 6685, 42070, 351, 4738, 6276, 871, 326, 314, 423, 2982, 357, 40, 1839, 470, 910, 416, 4150, 8, 3688, 284, 402, 271, 10899, 338, 12036, 13, 843, 523, 438, 14363, 10568, 852, 5729, 11331, 18893, 540, 438, 1169, 5114, 11835, 3724, 503, 11, 290, 11, 355, 9074, 13, 536, 5469, 550, 11001, 11, 262, 2756, 286, 366, 38, 271, 10899, 82, 1, 1816, 510, 13, 198, 198, 1026, 373, 407, 10597, 1115, 812, 1568, 326, 11, 287, 262, 1781, 286, 257, 1178, 2745, 6, 4686, 1359, 319, 262, 34686, 41976, 11, 340, 6451, 5091, 284, 502, 284, 4240, 1521, 402, 271, 10899, 550, 1813, 510, 465, 12036, 13, 1550, 14580, 11, 340, 1107, 373, 257, 29850, 1917, 13, 1675, 24456, 465, 3656, 561, 423, 587, 1165, 2562, 438, 14363, 3148, 1650, 1010, 550, 587, 6699, 262, 1540, 558, 286, 2282, 326, 9074, 13, 402, 271, 10899, 550, 366, 7109, 14655, 683, 866, 526, 1114, 9074, 13, 402, 271, 10899, 438, 292, 884, 438, 18108, 407, 11196, 10597, 3016, 257, 614, 706, 3619, 338, 10568, 550, 587, 2077, 13, 632, 1244, 307, 326, 339, 550, 6405, 607, 438, 20777, 339, 8288, 465, 10152, 438, 13893, 339, 1422, 470, 765, 284, 467, 319, 12036, 26, 475, 340, 561, 423, 587, 1327, 284, 5879, 326, 339, 550, 1813, 510, 465, 12036, 780, 339, 550, 6405, 607, 13, 198, 198, 5189, 1781, 11, 611, 673, 550, 407, 17901, 683, 866, 11, 673, 550, 8603, 11, 355, 4544, 9325, 701, 42397, 11, 4054, 284, 366, 26282, 683, 510, 1, 438, 7091, 550, 407, 2957, 683, 736, 284, 262, 1396, 417, 13, 1675, 1234, 262, 14093, 656, 465, 1021, 757, 438, 10919, 257, 410, 5040, 329, 257, 3656, 0, 887, 9074, 13, 402, 271, 10899, 4120, 284, 423, 595, 67, 1328, 340, 438, 392, 314, 2936, 340, 1244, 307, 3499, 284, 1064, 503, 1521, 13, 198, 198, 464, 748, 586, 652, 1204, 286, 262, 34686, 41976, 37733, 2346, 284, 884, 14177, 8233, 1020, 5768, 26, 290, 1719, 11, 319, 616, 835, 284, 22489, 40089, 11, 4978, 257, 19350, 286, 3619, 338, 3652, 436, 81, 5286, 8812, 2114, 1022, 262, 279, 1127, 11, 314, 550, 3589, 28068, 294, 1555, 262, 1306, 1110, 13, 198, 198, 40, 1043, 262, 3155, 379, 8887, 11061, 511, 18057, 12, 83, 6037, 26, 290, 9074, 13, 402, 271, 10899, 338, 7062, 373, 523, 2429, 498, 326, 11, 287, 262, 29543, 2745, 11, 314, 4752, 340, 6777, 13, 632, 373, 407, 326, 616, 2583, 408, 373, 366, 47914, 1298, 319, 326, 966, 314, 714, 423, 1813, 4544, 9325, 701, 262, 40830, 12719, 3874, 13, 632, 373, 655, 780, 673, 373, 4808, 1662, 62, 3499, 438, 361, 314, 743, 307, 41746, 12004, 262, 6473, 438, 5562, 314, 1043, 607, 523, 13, 1114, 3619, 11, 477, 465, 1204, 11, 550, 587, 11191, 416, 3499, 1466, 25, 484, 550, 26546, 1068, 465, 1242, 11, 340, 550, 587, 302, 1144, 287, 262, 3024, 12, 4803, 286, 511, 512, 1741, 13, 843, 340, 373, 4361, 5048, 425, 284, 3465, 644, 1245, 262, 366, 25124, 3101, 8137, 286, 16957, 1696, 414, 1, 357, 40, 9577, 4544, 9325, 701, 8, 373, 1719, 319, 683, 13, 198, 198, 40, 423, 4750, 326, 9074, 13, 402, 271, 10899, 373, 5527, 26, 290, 340, 373, 3393, 34953, 856, 326, 607, 5229, 373, 37895, 422, 428, 25179, 257, 19217, 475, 8904, 14676, 13, 632, 318, 11, 355, 257, 3896, 11, 262, 661, 508, 40987, 1637, 508, 651, 749, 503, 286, 340, 26, 290, 3619, 338, 19992, 31564, 286, 465, 3656, 338, 1263, 5236, 9343, 683, 11, 351, 281, 5585, 286, 2818, 922, 12, 49705, 11, 284, 21595, 1133, 340, 656, 5563, 286, 1242, 290, 13064, 13, 1675, 262, 6846, 11, 314, 1276, 751, 11, 339, 6150, 5365, 31655, 26, 475, 339, 373, 7067, 29396, 18443, 12271, 290, 45592, 12, 14792, 5986, 351, 257, 8839, 326, 7284, 35924, 262, 12306, 395, 4133, 13, 198, 198, 1, 26788, 338, 691, 12226, 318, 284, 1234, 8737, 656, 19133, 553, 373, 530, 286, 262, 7877, 72, 3150, 339, 8104, 866, 1973, 262, 37918, 411, 290, 8465, 286, 281, 33954, 271, 3973, 9899, 14678, 40556, 12, 11487, 11, 618, 11, 319, 257, 1568, 1110, 11, 314, 550, 757, 1057, 625, 422, 22489, 40089, 26, 290, 9074, 13, 402, 271, 10899, 11, 307, 3723, 319, 683, 11, 2087, 329, 616, 35957, 25, 366, 14295, 318, 523, 34813, 306, 8564, 284, 790, 1296, 286, 8737, 526, 198, 198, 43920, 3619, 0, 632, 550, 1464, 587, 465, 10030, 284, 423, 1466, 910, 884, 1243, 286, 683, 25, 262, 1109, 815, 307, 900, 866, 287, 1070, 268, 2288, 13, 1867, 7425, 502, 783, 373, 326, 11, 329, 262, 717, 640, 11, 339, 581, 4714, 262, 8216, 13, 314, 550, 1775, 683, 11, 523, 1690, 11, 1615, 3364, 739, 2092, 256, 7657, 438, 9776, 340, 262, 11644, 43778, 3465, 326, 26773, 606, 286, 511, 6799, 454, 30, 1400, 438, 1640, 11, 31414, 1576, 11, 340, 2627, 4156, 326, 339, 373, 16245, 286, 9074, 13, 402, 271, 10899, 438, 69, 623, 1576, 407, 284, 766, 607, 41793, 13, 632, 373, 465, 898, 41793, 339, 3947, 284, 307, 1592, 2259, 739, 438, 14363, 898, 9408, 355, 281, 2134, 329, 5482, 4447, 290, 753, 1072, 13, 198, 198, 1, 3666, 13674, 11, 1201, 314, 1053, 442, 17758, 12036, 661, 836, 470, 910, 326, 3404, 546, 502, 438, 9930, 910, 340, 546, 12622, 41379, 293, 553, 373, 465, 691, 5402, 11, 355, 339, 8278, 422, 262, 3084, 290, 336, 8375, 503, 4291, 262, 4252, 18250, 8812, 558, 13, 198, 198, 40, 27846, 706, 683, 11, 7425, 416, 465, 938, 1573, 13, 12622, 41379, 293, 373, 11, 287, 1109, 11, 5033, 262, 582, 286, 262, 2589, 438, 292, 3619, 2241, 11, 530, 1244, 1234, 340, 11, 550, 587, 262, 582, 286, 262, 1711, 13, 383, 7099, 6802, 373, 531, 284, 423, 7042, 2241, 379, 616, 1545, 338, 3625, 11, 290, 314, 14028, 611, 257, 256, 11912, 286, 35394, 739, 10724, 262, 6846, 338, 11428, 450, 67, 3299, 13, 887, 645, 438, 1640, 340, 373, 407, 10597, 706, 326, 1785, 326, 262, 4808, 13698, 10322, 6532, 62, 8263, 12, 9649, 550, 9258, 284, 3359, 511, 366, 8642, 521, 829, 526, 198, 198, 40, 2900, 284, 9074, 13, 402, 271, 10899, 11, 508, 550, 18459, 1068, 284, 1577, 257, 23844, 286, 7543, 284, 607, 599, 6321, 287, 262, 17423, 12, 3823, 13, 198, 198, 1, 5195, 4808, 10134, 62, 339, 442, 17758, 12036, 1701, 314, 1965, 25891, 13, 198, 198, 3347, 4376, 607, 26928, 351, 257, 9254, 286, 922, 12, 17047, 8167, 5975, 13, 198, 198, 1, 5812, 11, 339, 1595, 470, 4808, 14150, 62, 284, 783, 11, 345, 760, 26, 290, 314, 765, 683, 284, 2883, 2241, 553, 673, 531, 2407, 2391, 13, 198, 198, 40, 3114, 546, 262, 40894, 2330, 12, 6839, 11978, 2119, 11, 351, 663, 4808, 44769, 8270, 12, 332, 660, 62, 410, 1386, 20394, 262, 23755, 286, 262, 14005, 1801, 2093, 41160, 11, 290, 663, 45592, 12, 14792, 1613, 1424, 287, 19217, 24887, 13431, 13, 198, 198, 1, 19242, 339, 442, 17758, 465, 5986, 1165, 30, 314, 4398, 470, 1775, 257, 2060, 530, 287, 262, 2156, 526, 198, 198, 32, 3731, 17979, 286, 32315, 12606, 9074, 13, 402, 271, 10899, 338, 1280, 954, 36368, 13, 366, 1026, 338, 465, 11441, 48740, 11, 345, 760, 13, 679, 1139, 484, 821, 407, 4197, 284, 423, 546, 26, 339, 338, 1908, 606, 477, 1497, 2845, 530, 438, 1820, 18560, 438, 392, 326, 314, 423, 284, 1394, 26148, 526, 198, 198, 6653, 11441, 48740, 438, 14295, 338, 48740, 546, 465, 5986, 30, 2011, 20136, 373, 3957, 588, 262, 26394, 12, 301, 971, 13, 314, 531, 10722, 292, 2280, 284, 616, 2583, 408, 25, 366, 40, 1276, 1107, 766, 534, 18560, 11, 345, 760, 526, 198, 198, 3347, 27846, 503, 2048, 4628, 24882, 379, 262, 8812, 558, 810, 607, 5229, 11, 21081, 782, 278, 287, 257, 14263, 276, 5118, 11, 550, 6578, 257, 24518, 290, 7428, 262, 3394, 20096, 39047, 338, 1182, 1022, 465, 14475, 13, 198, 198, 1, 5779, 11, 1282, 981, 339, 338, 407, 2045, 553, 673, 531, 11, 351, 257, 6487, 326, 3088, 284, 7808, 607, 10927, 1108, 26, 290, 314, 3940, 607, 1022, 262, 30623, 2295, 49406, 286, 262, 6899, 11, 290, 510, 262, 3094, 16046, 351, 1059, 430, 12, 66, 12375, 299, 20896, 82, 24357, 1871, 12734, 379, 1123, 9581, 13, 198, 198, 818, 262, 5391, 76, 395, 5228, 286, 607, 275, 2778, 10840, 11, 10371, 257, 1534, 4241, 286, 19217, 290, 18876, 5563, 11, 9174, 530, 286, 262, 5385, 41186, 39614, 1386, 11, 287, 262, 13203, 5482, 1044, 276, 5739, 13, 383, 5019, 19001, 286, 262, 5739, 1444, 510, 477, 402, 271, 10899, 338, 1613, 0, 198, 198, 27034, 13, 402, 271, 10899, 9859, 736, 262, 4324, 12, 66, 3325, 1299, 11, 3888, 7263, 257, 4808, 73, 446, 259, 13235, 62, 1336, 286, 11398, 35560, 1000, 292, 11, 7121, 281, 3211, 12, 16337, 1497, 11, 290, 531, 25, 366, 1532, 345, 1302, 994, 345, 460, 655, 6687, 284, 766, 340, 13, 314, 550, 340, 625, 262, 24818, 417, 12, 12239, 11, 475, 339, 3636, 470, 1309, 340, 2652, 526, 198, 198, 5297, 438, 40, 714, 655, 6687, 284, 766, 340, 438, 1169, 717, 18560, 286, 3619, 338, 314, 550, 1683, 550, 284, 14022, 616, 2951, 625, 0, 19672, 484, 550, 262, 1295, 286, 15393, 438, 16706, 262, 4318, 6103, 287, 257, 14005, 7872, 393, 4808, 13698, 10322, 6532, 62, 8263, 12, 3823, 11, 393, 257, 36364, 1396, 417, 4624, 523, 326, 340, 1718, 262, 1657, 832, 41160, 286, 1468, 9932, 316, 666, 966, 13, 383, 517, 12949, 1295, 2627, 262, 4286, 1365, 26, 1865, 11, 355, 616, 2951, 6348, 23840, 284, 262, 2063, 12, 2971, 11, 477, 262, 16704, 14482, 1625, 503, 438, 439, 262, 10818, 20597, 32192, 355, 2709, 330, 871, 11, 262, 15910, 286, 16153, 312, 328, 3780, 416, 543, 11, 351, 884, 2784, 9830, 5032, 11, 339, 5257, 284, 36583, 3241, 422, 262, 1103, 1597, 286, 262, 4286, 284, 617, 2495, 11331, 2768, 590, 286, 3703, 13, 9074, 13, 402, 271, 10899, 11, 17728, 257, 8500, 4417, 284, 670, 319, 438, 15464, 11, 355, 340, 547, 11, 523, 16857, 262, 4469, 286, 607, 898, 4286, 438, 18108, 26269, 5223, 287, 281, 8468, 4922, 284, 262, 3359, 286, 428, 3991, 4118, 84, 16579, 13, 383, 4286, 373, 530, 286, 3619, 338, 366, 11576, 395, 553, 355, 465, 21099, 3808, 561, 423, 1234, 340, 438, 270, 7997, 11, 319, 465, 636, 11, 257, 29844, 286, 12749, 11, 257, 22791, 278, 286, 32375, 11, 257, 22486, 11, 965, 2860, 1359, 290, 965, 1397, 11, 326, 14516, 530, 286, 262, 33125, 12, 565, 593, 338, 25304, 4040, 284, 10303, 257, 17972, 13, 632, 1138, 11, 287, 1790, 11, 379, 790, 966, 262, 3512, 286, 14081, 2415, 284, 307, 13055, 366, 11576, 306, 1, 780, 673, 373, 10032, 286, 852, 13055, 366, 34751, 306, 1, 438, 392, 1865, 407, 284, 4425, 281, 22037, 286, 262, 32073, 13, 198, 198, 1, 1026, 338, 262, 938, 339, 13055, 11, 345, 760, 553, 9074, 13, 402, 271, 10899, 531, 351, 27322, 540, 11293, 13, 366, 464, 938, 475, 530, 553, 673, 19267, 5223, 438, 1, 4360, 262, 584, 1595, 470, 954, 11, 780, 339, 6572, 340, 526, 198, 198, 1, 49174, 276, 340, 1701, 314, 373, 546, 284, 1061, 510, 428, 18437, 618, 314, 2982, 257, 2366, 9662, 290, 2497, 3619, 2241, 319, 262, 11387, 13, 198, 198, 1722, 339, 6204, 612, 11, 465, 2832, 287, 262, 16511, 286, 465, 11555, 303, 7821, 13209, 11, 262, 7888, 7586, 9813, 286, 4190, 7121, 736, 422, 465, 2330, 22645, 11, 465, 10904, 4252, 6236, 429, 25839, 9230, 808, 276, 416, 257, 8212, 326, 13663, 262, 9040, 286, 257, 2116, 12, 10414, 738, 285, 23968, 4891, 11, 314, 2936, 284, 644, 257, 4922, 339, 550, 262, 976, 3081, 355, 465, 5986, 438, 1169, 3081, 286, 2045, 1190, 4119, 81, 621, 339, 373, 13, 198, 198, 6653, 3656, 27846, 379, 683, 1207, 8344, 803, 306, 11, 475, 465, 2951, 21650, 1613, 607, 284, 262, 18560, 13, 198, 198, 1, 5246, 13, 8759, 2763, 2227, 284, 766, 340, 553, 673, 2540, 11, 355, 611, 2859, 3500, 5223, 13, 679, 28271, 465, 12450, 11, 991, 16755, 13, 198, 198, 1, 5812, 11, 8759, 2763, 1043, 502, 503, 890, 2084, 553, 339, 531, 15376, 26, 788, 11, 6427, 465, 3211, 832, 6164, 25, 366, 16773, 290, 766, 262, 1334, 286, 262, 2156, 526, 198, 198, 1544, 3751, 340, 284, 502, 351, 257, 1611, 286, 24354, 20154, 11293, 25, 262, 7837, 12, 9649, 11, 262, 5486, 12, 83, 29080, 11, 262, 6576, 12, 565, 418, 1039, 11, 262, 4057, 2655, 12, 8439, 274, 438, 439, 262, 3716, 7106, 6637, 286, 262, 45172, 338, 5928, 3773, 13, 843, 8797, 616, 4240, 3432, 262, 2938, 17547, 339, 531, 11, 9644, 503, 465, 7721, 257, 1310, 25, 366, 5297, 11, 314, 1107, 836, 470, 766, 703, 661, 6687, 284, 2107, 1231, 326, 526, 198, 198, 5779, 438, 270, 373, 655, 262, 886, 530, 1244, 423, 1674, 15898, 329, 683, 13, 5514, 339, 373, 11, 832, 340, 477, 290, 287, 15275, 286, 340, 477, 438, 292, 339, 550, 587, 832, 11, 290, 287, 15275, 286, 11, 465, 5986, 438, 568, 22665, 11, 523, 23332, 11, 523, 595, 18052, 11, 326, 530, 890, 276, 284, 3960, 503, 25, 366, 3856, 44455, 351, 534, 24638, 2474, 355, 1752, 530, 550, 890, 276, 284, 910, 25, 366, 3856, 44455, 351, 534, 670, 2474, 198, 198, 1537, 11, 351, 262, 3960, 319, 616, 11914, 11, 616, 13669, 6989, 281, 10059, 2198, 13, 198, 198, 1, 1212, 318, 616, 898, 49451, 553, 339, 531, 11, 3756, 502, 656, 257, 3223, 8631, 2119, 379, 262, 886, 286, 262, 781, 273, 312, 410, 12523, 13, 632, 373, 6616, 290, 7586, 290, 11620, 88, 25, 645, 366, 34435, 8172, 645, 865, 291, 12, 64, 12, 1671, 330, 11, 4844, 286, 262, 1633, 286, 24380, 329, 20728, 287, 257, 4286, 10273, 438, 29370, 477, 11, 645, 1551, 1051, 286, 1683, 1719, 587, 973, 355, 257, 8034, 13, 198, 198, 464, 1109, 3181, 1363, 284, 502, 262, 4112, 957, 1483, 286, 3619, 338, 2270, 351, 465, 1468, 1204, 13, 198, 198, 1, 3987, 470, 345, 1683, 45553, 903, 351, 7521, 597, 517, 1701, 314, 1965, 11, 991, 2045, 546, 329, 257, 12854, 286, 884, 3842, 13, 198, 198, 1, 12295, 553, 339, 531, 11589, 13, 198, 198, 1, 5574, 1660, 12, 49903, 438, 273, 2123, 10813, 1701, 198, 198, 6653, 6563, 2951, 6348, 5391, 11, 290, 465, 25839, 279, 3021, 257, 1310, 739, 511, 22665, 4252, 10899, 13, 198, 198, 1, 12295, 892, 286, 340, 11, 616, 13674, 5891, 438, 1092, 517, 621, 611, 314, 1549, 1239, 12615, 257, 14093, 526, 198, 198, 1870, 465, 8216, 1297, 502, 287, 257, 7644, 326, 339, 1239, 1807, 286, 1997, 2073, 13, 198, 198, 40, 3888, 1497, 11, 43045, 21100, 416, 616, 10059, 9412, 26, 290, 355, 314, 2900, 11, 616, 4151, 3214, 319, 257, 1402, 4286, 2029, 262, 24818, 417, 12, 12239, 438, 1169, 691, 2134, 7163, 262, 8631, 26210, 3425, 9417, 286, 262, 2119, 13, 198, 198, 1, 5812, 11, 416, 449, 659, 2474, 314, 531, 13, 198, 198, 1026, 373, 257, 17548, 286, 257, 50085, 438, 272, 1468, 10032, 50085, 11, 5055, 287, 262, 6290, 739, 257, 3355, 13, 198, 198, 1, 3886, 449, 659, 438, 64, 520, 5493, 2474, 314, 16896, 13, 198, 198, 1544, 373, 10574, 26, 475, 314, 2936, 683, 1969, 2157, 502, 11, 12704, 257, 1310, 2952, 13, 198, 198, 1, 2061, 257, 4240, 0, 14446, 351, 257, 8667, 3951, 438, 4360, 319, 45697, 19369, 13, 921, 9670, 28022, 11, 810, 750, 345, 651, 340, 1701, 198, 198, 1544, 9373, 6364, 25, 366, 27034, 13, 520, 5493, 2921, 340, 284, 502, 526, 198, 198, 1, 10910, 438, 40, 1422, 470, 760, 345, 772, 2993, 262, 520, 5493, 82, 13, 679, 373, 884, 281, 1167, 2588, 856, 607, 2781, 526, 198, 198, 1, 40, 1422, 470, 438, 83, 359, 706, 13, 764, 764, 764, 1375, 1908, 329, 502, 284, 7521, 683, 618, 339, 373, 2636, 526, 198, 198, 1, 2215, 339, 373, 2636, 30, 921, 1701, 198, 198, 40, 1276, 423, 1309, 257, 1310, 1165, 881, 40642, 972, 6654, 832, 616, 5975, 11, 329, 339, 9373, 351, 257, 1207, 8344, 803, 6487, 25, 366, 5297, 438, 7091, 338, 281, 12659, 2829, 1122, 11, 345, 760, 11, 9074, 13, 520, 5493, 13, 2332, 691, 2126, 373, 284, 423, 683, 1760, 416, 257, 38378, 34537, 438, 993, 11, 3595, 520, 5493, 0, 1375, 1807, 340, 262, 1654, 301, 835, 286, 46431, 465, 27951, 438, 1659, 10833, 340, 319, 257, 1308, 27461, 1171, 13, 843, 379, 262, 2589, 314, 373, 4808, 1169, 62, 38378, 34537, 526, 198, 198, 1, 10910, 11, 3595, 520, 5493, 438, 292, 345, 910, 13, 8920, 4808, 5562, 62, 465, 2106, 1701, 198, 198, 1, 2504, 373, 465, 2106, 13, 1375, 4762, 287, 683, 11, 26996, 798, 287, 683, 438, 273, 1807, 673, 750, 13, 887, 673, 3521, 470, 6842, 407, 284, 423, 477, 262, 8263, 12, 9649, 351, 607, 13, 1375, 3521, 470, 6842, 262, 1109, 326, 11, 319, 1401, 77, 3929, 1528, 11, 530, 714, 1464, 651, 1474, 1576, 284, 766, 465, 5986, 13, 23676, 2415, 0, 1375, 338, 655, 257, 24225, 39136, 278, 329, 584, 21441, 13, 520, 5493, 318, 262, 691, 2187, 314, 1683, 2993, 526, 198, 198, 1, 1639, 1683, 2993, 30, 887, 345, 655, 531, 438, 1, 198, 198, 38, 271, 10899, 550, 257, 11040, 8212, 287, 465, 2951, 13, 198, 198, 1, 5812, 11, 314, 2993, 683, 11, 290, 339, 2993, 502, 438, 8807, 340, 3022, 706, 339, 373, 2636, 526, 198, 198, 40, 5710, 616, 3809, 43045, 13, 366, 2215, 673, 1908, 329, 345, 1701, 198, 198, 1, 5297, 438, 37121, 1035, 27339, 284, 262, 21296, 13, 1375, 2227, 683, 29178, 3474, 438, 392, 416, 502, 2474, 198, 198, 1544, 13818, 757, 11, 290, 9617, 736, 465, 1182, 284, 804, 510, 379, 262, 17548, 286, 262, 50085, 13, 366, 1858, 547, 1528, 618, 314, 3521, 470, 804, 379, 326, 1517, 438, 24089, 77, 470, 1986, 340, 13, 887, 314, 4137, 3589, 284, 1234, 340, 994, 26, 290, 783, 340, 338, 30703, 502, 438, 66, 1522, 502, 13, 1320, 338, 262, 1738, 1521, 314, 836, 470, 45553, 903, 597, 517, 11, 616, 13674, 8759, 2763, 26, 393, 2138, 520, 5493, 2241, 318, 262, 1738, 526, 198, 198, 1890, 262, 717, 640, 616, 21696, 20136, 546, 616, 15185, 2900, 656, 257, 2726, 6227, 284, 1833, 683, 1365, 13, 198, 198, 1, 40, 4601, 345, 1549, 1560, 502, 703, 340, 3022, 553, 314, 531, 13, 198, 198, 1544, 6204, 2045, 510, 379, 262, 17548, 11, 290, 665, 24297, 1022, 465, 9353, 257, 17779, 339, 550, 11564, 284, 1657, 13, 24975, 339, 2900, 3812, 502, 13, 198, 198, 1, 40, 1549, 2138, 588, 284, 1560, 345, 438, 13893, 314, 1053, 1464, 9885, 345, 286, 2376, 26927, 616, 670, 526, 198, 198, 40, 925, 257, 1207, 8344, 803, 18342, 11, 543, 339, 2469, 265, 1572, 351, 257, 922, 12, 17047, 8167, 32545, 13, 198, 198, 1, 5812, 11, 314, 1422, 470, 1337, 257, 14787, 618, 314, 4762, 287, 3589, 438, 392, 783, 340, 338, 281, 2087, 9839, 1022, 514, 2474, 198, 198, 1544, 13818, 4622, 11, 1231, 35987, 11, 290, 7121, 530, 286, 262, 2769, 3211, 12, 49655, 2651, 13, 366, 1858, 25, 787, 3511, 6792, 438, 392, 994, 389, 262, 33204, 345, 588, 526, 198, 198, 1544, 4624, 606, 379, 616, 22662, 290, 3767, 284, 27776, 510, 290, 866, 262, 2119, 11, 12225, 783, 290, 788, 11061, 262, 4286, 13, 198, 198, 1, 2437, 340, 3022, 30, 314, 460, 1560, 345, 287, 1936, 2431, 438, 392, 340, 1422, 470, 1011, 881, 2392, 284, 1645, 13, 764, 764, 764, 314, 460, 3505, 783, 703, 6655, 290, 10607, 314, 373, 618, 314, 1392, 9074, 13, 520, 5493, 338, 3465, 13, 3226, 1781, 11, 2769, 866, 11, 314, 550, 1464, 4808, 31985, 62, 612, 373, 645, 530, 588, 683, 438, 8807, 314, 550, 3750, 351, 262, 4269, 11, 22211, 262, 6678, 40315, 10455, 546, 683, 11, 10597, 314, 2063, 1392, 284, 892, 339, 373, 257, 5287, 11, 530, 286, 262, 1611, 326, 389, 1364, 2157, 13, 2750, 449, 659, 11, 290, 339, 4808, 9776, 62, 1364, 2157, 438, 13893, 339, 550, 1282, 284, 2652, 0, 383, 1334, 286, 514, 550, 284, 1309, 6731, 307, 17676, 1863, 393, 467, 739, 11, 475, 339, 373, 1029, 2029, 262, 1459, 438, 261, 45697, 19369, 11, 355, 345, 910, 13, 198, 198, 1, 5779, 11, 314, 1816, 572, 284, 262, 2156, 287, 616, 749, 34372, 10038, 438, 34330, 3888, 11, 4453, 20927, 502, 11, 379, 262, 3108, 418, 286, 3595, 520, 5493, 338, 3451, 286, 5287, 852, 37492, 416, 262, 13476, 286, 616, 12036, 683, 0, 3226, 1781, 314, 4001, 284, 466, 262, 4286, 329, 2147, 438, 40, 1297, 9074, 13, 520, 5493, 523, 618, 673, 2540, 284, 336, 321, 647, 1223, 546, 607, 8098, 13, 314, 3505, 1972, 572, 257, 40426, 10956, 9546, 546, 262, 15393, 852, 4808, 3810, 62, 438, 1219, 11, 314, 373, 19716, 306, 11, 616, 13674, 8759, 2763, 0, 314, 373, 24380, 284, 3589, 588, 530, 286, 616, 898, 1650, 1010, 13, 198, 198, 1, 6423, 314, 373, 2077, 510, 290, 1364, 3436, 351, 683, 13, 314, 550, 1908, 477, 616, 20348, 287, 5963, 11, 290, 314, 550, 691, 284, 900, 510, 262, 1396, 417, 290, 651, 284, 670, 13, 679, 550, 587, 2636, 691, 8208, 12, 14337, 2250, 11, 290, 339, 3724, 6451, 11, 286, 2612, 4369, 11, 523, 326, 612, 550, 587, 645, 15223, 670, 286, 8166, 438, 14363, 1986, 373, 1598, 290, 36519, 13, 314, 550, 1138, 683, 1752, 393, 5403, 11, 812, 878, 11, 290, 1807, 683, 32081, 290, 44852, 88, 13, 2735, 314, 2497, 326, 339, 373, 21840, 13, 198, 198, 1, 40, 373, 9675, 379, 717, 11, 351, 257, 6974, 19713, 14676, 25, 9675, 284, 423, 616, 1021, 319, 884, 257, 705, 32796, 2637, 3244, 465, 6283, 1204, 12, 46965, 9449, 2540, 284, 2689, 502, 24506, 306, 438, 292, 314, 10226, 262, 1182, 287, 314, 2936, 355, 611, 339, 547, 4964, 502, 466, 340, 13, 383, 18098, 373, 3940, 416, 262, 1807, 25, 611, 339, 4808, 22474, 62, 4964, 502, 11, 644, 561, 339, 910, 284, 616, 835, 286, 1762, 30, 2011, 29483, 2540, 284, 467, 257, 1310, 4295, 438, 40, 2936, 10927, 290, 8627, 13, 198, 198, 1, 7454, 11, 618, 314, 3114, 510, 11, 314, 3947, 284, 766, 257, 8212, 2157, 465, 1969, 12768, 680, 21213, 438, 292, 611, 339, 550, 262, 3200, 11, 290, 547, 28297, 2241, 416, 4769, 340, 736, 422, 502, 13, 1320, 41851, 515, 502, 991, 517, 13, 383, 3200, 30, 4162, 11, 314, 550, 257, 3200, 2861, 8208, 286, 465, 0, 314, 37901, 379, 262, 21978, 44896, 11, 290, 3088, 617, 286, 616, 49025, 5330, 15910, 13, 887, 484, 4054, 502, 11, 484, 1067, 11137, 13, 314, 2497, 326, 339, 2492, 470, 4964, 262, 905, 88, 10340, 438, 40, 3521, 470, 11786, 465, 3241, 26, 339, 655, 4030, 465, 2951, 319, 262, 1327, 22674, 1022, 13, 5845, 547, 262, 3392, 314, 550, 1464, 427, 343, 9091, 11, 393, 5017, 510, 351, 617, 9105, 7521, 13, 843, 703, 339, 2497, 832, 616, 7363, 0, 198, 198, 1, 40, 3114, 510, 757, 11, 290, 4978, 6504, 286, 326, 17548, 286, 262, 50085, 10938, 319, 262, 3355, 1474, 465, 3996, 13, 2399, 3656, 1297, 502, 20875, 340, 373, 262, 938, 1517, 339, 550, 1760, 438, 3137, 257, 3465, 2077, 351, 257, 17275, 1021, 11, 618, 339, 373, 866, 287, 6245, 684, 10695, 20222, 422, 257, 2180, 2612, 1368, 13, 2329, 257, 3465, 0, 887, 340, 4952, 465, 2187, 2106, 13, 1318, 389, 812, 286, 5827, 40987, 913, 30802, 287, 790, 1627, 13, 317, 582, 508, 550, 1509, 388, 351, 262, 1459, 714, 1239, 423, 4499, 326, 18680, 510, 12, 5532, 14000, 13, 764, 764, 764, 198, 198, 1, 40, 2900, 736, 284, 616, 670, 11, 290, 1816, 319, 39136, 278, 290, 285, 4185, 1359, 26, 788, 314, 3114, 379, 262, 50085, 757, 13, 314, 2497, 326, 11, 618, 520, 5493, 8104, 287, 262, 717, 14000, 11, 339, 2993, 655, 644, 262, 886, 561, 307, 13, 679, 550, 17273, 465, 2426, 11, 19233, 340, 11, 11027, 515, 340, 13, 1649, 550, 314, 1760, 326, 351, 597, 286, 616, 1243, 30, 1119, 8020, 470, 587, 4642, 286, 502, 438, 40, 550, 655, 8197, 606, 13, 764, 764, 764, 198, 198, 1, 39, 648, 340, 11, 8759, 2763, 11, 351, 326, 1986, 4964, 502, 314, 3521, 470, 466, 1194, 14000, 13, 383, 8631, 3872, 373, 11, 314, 1422, 470, 760, 810, 284, 1234, 340, 438, 62, 40, 550, 1239, 1900, 44807, 5514, 11, 351, 616, 1650, 1010, 290, 616, 1171, 11, 257, 905, 88, 22870, 286, 9568, 5017, 510, 262, 1109, 438, 40, 655, 9617, 7521, 656, 511, 6698, 13, 764, 764, 764, 3894, 11, 7521, 373, 262, 530, 7090, 883, 2636, 2951, 714, 766, 832, 438, 3826, 3892, 284, 262, 2006, 20212, 19369, 14638, 13, 2094, 470, 345, 760, 703, 11, 287, 3375, 257, 3215, 3303, 11, 772, 6562, 1473, 11, 530, 1139, 2063, 262, 640, 407, 644, 530, 3382, 284, 475, 644, 530, 460, 30, 3894, 438, 5562, 373, 262, 835, 314, 13055, 26, 290, 355, 339, 3830, 612, 290, 7342, 502, 11, 262, 1517, 484, 1444, 616, 705, 23873, 2350, 6, 14707, 588, 257, 2156, 286, 4116, 13, 679, 1422, 470, 10505, 263, 11, 345, 1833, 11, 3595, 520, 5493, 438, 258, 655, 3830, 612, 12703, 4964, 11, 290, 319, 465, 11914, 11, 832, 262, 12768, 21213, 11, 314, 3947, 284, 3285, 262, 1808, 25, 705, 8491, 345, 1654, 345, 760, 810, 345, 821, 2406, 503, 8348, 198, 198, 1, 1532, 314, 714, 423, 13055, 326, 1986, 11, 351, 326, 1808, 319, 340, 11, 314, 815, 423, 1760, 257, 1049, 1517, 13, 383, 1306, 6000, 1517, 373, 284, 766, 326, 314, 3521, 470, 438, 392, 326, 11542, 373, 1813, 502, 13, 887, 11, 11752, 11, 379, 326, 5664, 11, 8759, 2763, 11, 373, 612, 1997, 319, 4534, 314, 3636, 470, 423, 1813, 284, 423, 520, 5493, 6776, 878, 502, 11, 290, 284, 3285, 683, 910, 25, 705, 1026, 338, 407, 1165, 2739, 438, 40, 1183, 905, 345, 703, 30960, 198, 198, 1, 1026, 4808, 9776, 62, 1165, 2739, 438, 270, 561, 423, 587, 11, 772, 611, 339, 1549, 587, 6776, 13, 314, 11856, 510, 616, 20348, 11, 290, 1816, 866, 290, 1297, 9074, 13, 520, 5493, 13, 3226, 1781, 314, 1422, 470, 1560, 607, 4808, 5562, 62, 438, 270, 561, 423, 587, 8312, 284, 607, 13, 314, 2391, 531, 314, 3521, 470, 7521, 683, 11, 326, 314, 373, 1165, 3888, 13, 1375, 2138, 8288, 262, 2126, 438, 7091, 338, 523, 14348, 0, 632, 373, 326, 326, 925, 607, 1577, 502, 262, 50085, 13, 887, 673, 373, 22121, 9247, 379, 407, 1972, 262, 18560, 438, 7091, 750, 523, 765, 683, 705, 28060, 6, 416, 617, 530, 905, 88, 0, 1629, 717, 314, 373, 7787, 673, 3636, 470, 1309, 502, 572, 438, 392, 379, 616, 266, 896, 6, 886, 314, 5220, 41379, 293, 13, 3363, 11, 340, 373, 314, 508, 2067, 41379, 293, 25, 314, 1297, 9074, 13, 520, 5493, 339, 373, 262, 705, 4976, 6, 582, 11, 290, 673, 1297, 8276, 2073, 11, 290, 523, 340, 1392, 284, 307, 2081, 13, 764, 764, 764, 843, 339, 13055, 520, 5493, 1231, 1592, 2259, 26, 290, 673, 9174, 262, 4286, 1871, 607, 5229, 338, 1243, 13, 764, 764, 22135, 198, 198, 1544, 45111, 2241, 866, 287, 262, 3211, 12, 16337, 1474, 6164, 11, 8104, 736, 465, 1182, 11, 290, 47425, 278, 465, 5101, 11061, 340, 11, 3114, 510, 379, 262, 4286, 2029, 262, 18205, 1681, 12, 12239, 13, 198, 198, 1, 40, 588, 284, 14996, 326, 520, 5493, 2241, 561, 423, 1813, 340, 284, 502, 11, 611, 339, 1549, 587, 1498, 284, 910, 644, 339, 1807, 326, 1110, 526, 198, 198, 1870, 11, 287, 3280, 284, 257, 1808, 314, 1234, 2063, 12, 1326, 3147, 1146, 438, 1, 44140, 757, 1701, 339, 30050, 503, 13, 366, 2215, 262, 530, 1517, 326, 6774, 502, 6609, 1474, 683, 318, 326, 314, 2993, 1576, 284, 2666, 572, 1701, 198, 198, 1544, 6204, 510, 290, 8104, 465, 1021, 319, 616, 8163, 351, 257, 6487, 13, 366, 10049, 262, 21296, 286, 340, 318, 326, 314, 4808, 321, 62, 991, 12036, 438, 20777, 41379, 293, 338, 1804, 340, 329, 502, 0, 383, 520, 5493, 82, 1302, 3436, 11, 290, 1645, 1752, 438, 4360, 612, 338, 645, 42393, 803, 674, 1611, 286, 1242, 526]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "print(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c6943f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "977fabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "891fa9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "757ad66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834bbfe",
   "metadata": {},
   "source": [
    "- We use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">\n",
    "\n",
    "- Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec081d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must be at least equal to max_length+1\"\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd3fc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer=tiktoken.get_encoding(\"r50k_base\")\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45520f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",'r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb3af6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3493f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,batch_size=8,max_length=4,stride=4,shuffle=False\n",
    ")\n",
    "inputs, targets = next(iter(dataloader))\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\",targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065d142",
   "metadata": {},
   "source": [
    "## Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c149f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1bbb8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46f4b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "555454b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad939745",
   "metadata": {},
   "source": [
    "## Encoding word positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92c40732",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8bad732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "inputs, targets = next(iter(dataloader))\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "715c5350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n",
      "         [ 0.4481,  0.2536, -0.2655,  ...,  0.4997, -1.1991, -1.1844],\n",
      "         [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n",
      "         [ 0.9457,  0.8657,  1.6191,  ..., -0.4544, -0.7460,  0.3483]],\n",
      "\n",
      "        [[ 1.5460,  1.7368, -0.7848,  ..., -0.1004,  0.8584, -0.3421],\n",
      "         [-1.8622, -0.1914, -0.3812,  ...,  1.1220, -0.3496,  0.6091],\n",
      "         [ 1.9847, -0.6483, -0.1415,  ..., -0.3841, -0.9355,  1.4478],\n",
      "         [ 0.9647,  1.2974, -1.6207,  ...,  1.1463,  1.5797,  0.3969]],\n",
      "\n",
      "        [[-0.7713,  0.6572,  0.1663,  ..., -0.8044,  0.0542,  0.7426],\n",
      "         [ 0.8046,  0.5047,  1.2922,  ...,  1.4648,  0.4097,  0.3205],\n",
      "         [ 0.0795, -1.7636,  0.5750,  ...,  2.1823,  1.8231, -0.3635],\n",
      "         [ 0.4267, -0.0647,  0.5686,  ..., -0.5209,  1.3065,  0.8473]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6156,  0.9610, -2.6437,  ..., -0.9645,  1.0888,  1.6383],\n",
      "         [-0.3985, -0.9235, -1.3163,  ..., -1.1582, -1.1314,  0.9747],\n",
      "         [ 0.6089,  0.5329,  0.1980,  ..., -0.6333, -1.1023,  1.6292],\n",
      "         [ 0.3677, -0.1701, -1.3787,  ...,  0.7048,  0.5028, -0.0573]],\n",
      "\n",
      "        [[-0.1279,  0.6154,  1.7173,  ...,  0.3789, -0.4752,  1.5258],\n",
      "         [ 0.4861, -1.7105,  0.4416,  ...,  0.1475, -1.8394,  1.8755],\n",
      "         [-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
      "         [ 0.2002, -0.7605, -1.5170,  ..., -0.0305, -0.3656, -0.1398]],\n",
      "\n",
      "        [[-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
      "         [-0.0632, -0.6548, -1.0296,  ..., -0.9538, -0.5026, -0.1128],\n",
      "         [ 0.6032,  0.8983,  2.0722,  ...,  1.5242,  0.2030, -0.3002],\n",
      "         [ 1.1274, -0.1082, -0.2195,  ...,  0.5059, -1.8138, -0.0700]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53040052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0f131",
   "metadata": {},
   "source": [
    "- GPT-2 uses absolute position embeddings, so we just create another embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3b1dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52207fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.2288,  0.5619,  0.8286,  ..., -0.6272, -0.2987,  0.8900],\n",
      "         [ 2.0903, -0.4664, -0.0593,  ...,  0.9115, -1.0493, -1.6473],\n",
      "         [-0.7158, -0.8304,  1.2494,  ...,  2.3952,  1.8773,  0.8051],\n",
      "         [ 0.2703,  0.4029,  3.0514,  ...,  0.3595, -1.4548,  0.8310]],\n",
      "\n",
      "        [[ 3.2835,  1.1749, -1.4150,  ..., -0.3281,  2.4332,  0.6924],\n",
      "         [-0.2199, -0.9114, -0.1750,  ...,  1.5337, -0.1998,  0.1462],\n",
      "         [ 1.5197, -1.4240,  0.4391,  ...,  1.0494, -1.4318,  2.3057],\n",
      "         [ 0.2893,  0.8346, -0.1884,  ...,  1.9602,  0.8709,  0.8796]],\n",
      "\n",
      "        [[ 0.9662,  0.0952, -0.4640,  ..., -1.0320,  1.6290,  1.7771],\n",
      "         [ 2.4468, -0.2154,  1.4984,  ...,  1.8766,  0.5595, -0.1423],\n",
      "         [-0.3856, -2.5393,  1.1556,  ...,  3.6157,  1.3267,  0.4944],\n",
      "         [-0.2487, -0.5275,  2.0009,  ...,  0.2930,  0.5977,  1.3300]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1219,  0.3991, -3.2740,  ..., -1.1921,  2.6637,  2.6728],\n",
      "         [ 1.2438, -1.6436, -1.1101,  ..., -0.7464, -0.9816,  0.5118],\n",
      "         [ 0.1439, -0.2428,  0.7786,  ...,  0.8001, -1.5986,  2.4871],\n",
      "         [-0.3077, -0.6329,  0.0536,  ...,  1.5188, -0.2060,  0.4254]],\n",
      "\n",
      "        [[ 1.6095,  0.0535,  1.0871,  ...,  0.1512,  1.0996,  2.5603],\n",
      "         [ 2.1284, -2.4306,  0.6478,  ...,  0.5593, -1.6896,  1.4126],\n",
      "         [-1.4224, -0.0750,  1.9386,  ...,  3.3712, -2.4016, -0.3237],\n",
      "         [-0.4752, -1.2234, -0.0847,  ...,  0.7834, -1.0744,  0.3429]],\n",
      "\n",
      "        [[ 0.7802,  0.1387,  0.7277,  ...,  1.7101, -0.3304, -0.1471],\n",
      "         [ 1.5791, -1.3749, -0.8234,  ..., -0.5420, -0.3528, -0.5756],\n",
      "         [ 0.1382,  0.1226,  2.6528,  ...,  2.9576, -0.2933,  0.5577],\n",
      "         [ 0.4520, -0.5711,  1.2128,  ...,  1.3198, -2.5226,  0.4127]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embedding = token_embeddings + pos_embedding\n",
    "print(input_embedding)\n",
    "print(input_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a6504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
